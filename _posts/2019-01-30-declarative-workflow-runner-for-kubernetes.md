---
layout: post
title:  Data Pipelines in Kubernetes for research workflows
subtitle: Declarative workflow runner for Kubernetes
date:   2019-01-30 23:00:00
categories: [machine-learning]
---

[code](https://github.com/project-workflow-kubernetes/) and [full report](http://bit.ly/2HFqCW0).

Reproducibility is one of the key aspects to develop reliable research. The swift grow of new technologies to store and process data have been pushing limits and helping in data analysis. Cloud computing systems, such as Kubernetes, help in the distribution and orchestration of data analysis pipelines or workflows. Some of these pipelines render an enormous amount of data and it is compelling to perform efficient computation. 

A job is a set of tasks that need to be executed in a predefined order. Tasks depends on data and data might be generated by previous tasks and it leads to indirect dependencies among tasks. Thus, a job can be modeled by a DAG (directed acyclic graph) where the nodes fall into two distinct categories, task or data. On the order hand, edges appears just between data and tasks and its direction determines if the data is an input or an output of the task. The figure below illustrates an example of a job and note that edges happen just between data and tasks. In this context data can be a table, a chart, a set of parameters, a single number representing the accuracy of a model for example.

![job](/resources/images/job.png)


In order to build our layer that is able to handle wisely with alteration in tasks or data, it is important take in account the following aspects:

- **task1** and **task2** are independent and can be run in parallel or even in different machines

- **task3** depends on data generated by **task1** and **task2** then it must wait until **data4, data5** are available

- if the file **data6** is modified, then just the **task3** should be impacted and re-run

- if the script of **task1** is modified, then **task1** and **task3** should be re-run
 
As cited above, the framework should be able to handle different environments and it means that **task1** can be written in *python2.7* and **task2** in *python3.6* or *R* for instance. It will give flexibility to use a vaster ecosystem of tools and libraries. In addition, it eliminates some drawback in scientific contribution since researchers with different backgrounds and skill set should be to work independently in the same project.


The goal of this blog post is to present a project that builds a layer on top of Kubernetes (using [Argo](https://argoproj.github.io/)) that is clever enough to discovery changes in these pipelines (whether in code or in data) and just re-run tasks that were impacted.

In summary we are looking for:

1. nice integration with Kubernetes
    
2. ability to run independent tasks in parallel 

3. be able to handle with tasks with different environments
    
4. be fault tolerant
    
5. run just tasks that were affected by some modification in data or code

6. have a simple interface with the final user (researchers)

Argo has already implemented 1, 2, 3 and 4 and our goal for this project is to build a layer that is able to handle with 5 and 6. This layer was designed to be a REST service deployed on Kubernetes cluster called **workflow-controler** and it was written in Python. **workflow-controler** has an endpoint **/run** that receives a POST request with *job_name* and *job_url* where *job_name* is a string containing the job's name and *job_url* is a string with the URL of the repository.




